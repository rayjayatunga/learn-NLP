{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhmYMZzsV4nh",
        "outputId": "09e93a6d-7303-43c0-94f3-441455329d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to python"
      ],
      "metadata": {
        "id": "qvF66BWoWpp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "27N_PjcaW76H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(spacy.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co6tteblXEJa",
        "outputId": "b98823fb-35ad-4cec-8622-f87d850eaf72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "fGOcz2Q6XGqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "1E2JvXdCX8AQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of breaking up original text into small units (tokens)\n",
        "\n",
        "Tokenization does this task by locating word boundaries, Ending point of a word and beginning of the next word is called word boundary"
      ],
      "metadata": {
        "id": "Hg7UCOLcZiRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = 'Apple is looking at buying U.K. startup for $1 billion!'\n",
        "s2 = 'Hello all, We are here to help you! email support@udemy.com or visit us at http://www.udemy.com!'\n",
        "s3 = '10km cab ride almost costs $20 in NYC'\n",
        "s4 = 'Let\\'s watch a movie together.'\n",
        "\n",
        "import spacy"
      ],
      "metadata": {
        "id": "CM0GAm6-Yg-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load pre-trained model of english language library\n",
        "model_1 = spacy.load(name= 'en_core_web_sm')"
      ],
      "metadata": {
        "id": "comawYpLZLdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "bS0YIKzeGFWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = spacy.load(name= 'en_core_web_md')"
      ],
      "metadata": {
        "id": "_sBCpPE8F0oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a doc object and explore tokens\n",
        "doc = model_1(s1)\n",
        "doc2 = model_1(s2)\n",
        "doc3 = model_1(s3)"
      ],
      "metadata": {
        "id": "E1T74e0zZzhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc3:\n",
        "  print(token.text)\n",
        "# spaCy will isolate punctuation that does not form an integral part of a word\n",
        "# like Quotation marks, commas and punctuation, they will be assigned their own token"
      ],
      "metadata": {
        "id": "_gg1Ep0CZ6e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIVhXuOTZ9eQ",
        "outputId": "9993a65a-88ac-43a5-fa53-03b15ed97f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ride"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "\n",
        "This involves finding the root word of a larger word. For example, 'running' may be traced back to 'run'"
      ],
      "metadata": {
        "id": "QvhwpV88JXO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "p_stemmer = PorterStemmer()\n",
        "s_stemmer = SnowballStemmer(language = 'english')\n",
        "words = ['run', 'runner','running','ran','runs','easily','fairly']"
      ],
      "metadata": {
        "id": "_8TyHF6AJoZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test out Porter Stemming\n",
        "for word in words:\n",
        "  root_word = p_stemmer.stem(word)\n",
        "  print(root_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPmFyDZDKSJk",
        "outputId": "15207a35-8d99-496f-b08a-8c690f14e226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "runner\n",
            "run\n",
            "ran\n",
            "run\n",
            "easili\n",
            "fairli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test out Snowball stemming\n",
        "for word in words:\n",
        "  snowstemmed_word = s_stemmer.stem(word)\n",
        "  print(snowstemmed_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XQiR2TbLAfS",
        "outputId": "71e8ad83-2919-4796-bc2c-eb6646dece63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "runner\n",
            "run\n",
            "ran\n",
            "run\n",
            "easili\n",
            "fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization\n",
        "Breaking a word down into its root meaning to identify similarites"
      ],
      "metadata": {
        "id": "S8SmIBg1L1k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp_model = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "7sBGf33DL6QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ozrHdEEOF5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp_model(\"The striped bats are hanging on their feet for best\")"
      ],
      "metadata": {
        "id": "tyVLfeEiMDQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc1:\n",
        "  print(token.text, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BseK7t7qMevy",
        "outputId": "bf6faab5-6d96-4cf2-fa2b-72ef5b051fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The the\n",
            "striped stripe\n",
            "bats bat\n",
            "are be\n",
            "hanging hang\n",
            "on on\n",
            "their their\n",
            "feet foot\n",
            "for for\n",
            "best good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop Words\n",
        "\n",
        "\"stop words\" are words that are filtered out before or after processing of natural language data (text). The rationale behind removing these words is that they are so common that they carry very little useful information about the actual meaning of a document. Including them could also drastically increase the size of the data to be processed, making computational tasks less efficient.\n",
        "\n",
        "Stop words typically include articles, prepositions, conjunctions, and sometimes the most common verbs and adjectives. Examples of stop words in English are \"the\", \"is\", \"at\", \"which\", and \"on\". However, the exact list of stop words can vary depending on the specific application or the processing needs"
      ],
      "metadata": {
        "id": "jbRQ56KlOL8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "model = spacy.load('en_core_web_sm')\n",
        "\n",
        "stop_words = model.Defaults.stop_words\n",
        "print(len(stop_words))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmk-Cnj2OnMx",
        "outputId": "42bee526-9a16-4733-a51d-f6c63733014c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326\n",
            "{'you', 'so', 'without', \"n't\", 'there', 'hereby', 'may', 'make', 'noone', 'anyone', 'below', 'off', 'us', 'for', 'from', 'myself', 'themselves', 'into', 'front', 'however', 'she', 'various', 'yourselves', '’s', 'had', 'herself', 'as', 'did', 'fifty', 'thence', 'thereupon', 'last', 'what', 'quite', 'onto', '‘re', 'itself', 'above', 'via', 'even', 'each', 'anyhow', 'someone', 'alone', \"'d\", 'bottom', 'somehow', 'own', 'becomes', 'its', 'all', 'between', 'seemed', 'is', 'regarding', 'empty', 'five', '‘m', 'was', 'call', 'least', 'full', 'whenever', 'give', 'herein', 'through', 'has', 'would', 'most', 'or', 'nowhere', 'forty', 'might', 'will', 'elsewhere', 'therein', 'see', 'put', 'others', 'beforehand', 'seems', 'many', 'nothing', 'next', 'two', 'before', 'keep', 'six', 'else', 'seeming', 'anything', 'out', 'whoever', 'either', 'are', 'few', 'just', 'me', 'hence', 'per', 'though', 'upon', 'after', 'does', 'if', 'serious', 'ten', 'ourselves', 'go', \"'re\", 'how', 'please', 'who', 'when', '’ll', 'being', 'moreover', 'himself', 'side', 'always', 'sometimes', 'than', 'not', 'very', 'ours', \"'s\", 'thereby', 'top', 'everything', 'amount', 'do', 'were', 'part', 're', 'a', 'he', 'to', 'used', 'mine', 'four', 'sixty', 'while', 'indeed', 'him', 'really', 'much', 'afterwards', 'which', 'whatever', 'during', 'less', 'say', '’m', 'with', 'in', 'sometime', 'under', 'one', 'but', 'some', 'perhaps', '‘ll', '‘ve', 'due', 'every', 'whereupon', 'by', 'anywhere', 'although', 'because', 'yet', 'neither', 'n‘t', 'this', 'why', 'along', 'namely', 'around', 'behind', 'also', 'throughout', 'using', 'other', 'of', 'made', \"'ll\", 'nobody', '‘d', 'get', 'since', 'something', 'formerly', 'again', 'beside', 'third', 'them', 'nevertheless', 'then', 'whom', 'except', 'up', 'unless', 'can', 'otherwise', 'name', 'that', \"'ve\", 'yourself', 'on', 'ca', 'together', 'once', 'whether', 'latter', 'enough', 'already', 'become', 'nine', 'take', 'against', 'they', 'within', 'been', 'toward', 'until', 'towards', 'now', 'anyway', 'these', 'only', 'latterly', 'your', 'none', 'whereby', 'we', 'whole', 'thru', 'it', 'thereafter', 'where', 'doing', 'twenty', 'and', 'n’t', 'whereafter', 'be', 'twelve', 'often', '’re', 'well', 'ever', 'both', 'about', 'still', 'almost', 'my', 'must', 'our', 'whence', 'former', 'eleven', 'have', 'fifteen', 'whereas', 'never', '’ve', 'his', 'rather', 'among', 'became', 'first', 'down', 'am', 'hereupon', 'mostly', 'hereafter', 'back', 'wherein', 'several', 'another', 'everyone', 'the', 'their', 'across', 'cannot', 'whose', 'an', 'show', 'i', 'seem', 'done', 'move', 'such', 'wherever', 'amongst', 'too', 'those', 'becoming', 'here', 'at', 'meanwhile', 'yours', 'everywhere', 'further', '’d', 'beyond', 'her', 'somewhere', 'thus', 'no', 'therefore', 'could', 'should', \"'m\", 'whither', 'besides', 'hers', 'hundred', 'same', 'any', 'nor', '‘s', 'over', 'more', 'eight', 'three'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Find out if a certain word is part of the model's vocabulary of stop words:\n",
        "model.vocab['ALWAYS'].is_stop # -> True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccqFfGesOzbc",
        "outputId": "fc1a4b8d-4391-48e7-a1ff-9c5152743c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Add a word to the stop word list\n",
        "model.Defaults.stop_words.add('asdf')"
      ],
      "metadata": {
        "id": "0dPoVkYDPS6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test if the model catches the new stop word\n",
        "test = 'Hello Arnold, asdf makes no sense to me'\n",
        "for word in test.split():\n",
        "  if (model.vocab[word].is_stop):\n",
        "    print(word)\n",
        "print(len(stop_words))\n",
        "#The model catches the new stop word. Printing len of the stop words to prove"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPUx73hIP-E0",
        "outputId": "7ccbe001-ad0b-484b-c587-56b47e34775c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "asdf\n",
            "no\n",
            "to\n",
            "me\n",
            "327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing stop words:\n",
        "model.Defaults.stop_words.remove('asdf')\n",
        "print(len(stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P44227UVQU2N",
        "outputId": "6b1684cb-5f81-40a2-f9d0-6e13e864f649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_0DabNkRgPi",
        "outputId": "ecac7ad7-14e2-4844-bf53-1bdd3276e9b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you', 'so', 'without', \"n't\", 'there', 'hereby', 'may', 'make', 'noone', 'anyone', 'below', 'off', 'us', 'for', 'from', 'myself', 'themselves', 'into', 'front', 'however', 'she', 'various', 'yourselves', '’s', 'had', 'herself', 'as', 'did', 'fifty', 'thence', 'thereupon', 'last', 'what', 'quite', 'onto', '‘re', 'itself', 'above', 'via', 'even', 'each', 'anyhow', 'someone', 'alone', \"'d\", 'bottom', 'somehow', 'own', 'becomes', 'its', 'all', 'between', 'seemed', 'is', 'regarding', 'empty', 'five', '‘m', 'was', 'call', 'least', 'full', 'whenever', 'give', 'herein', 'through', 'has', 'would', 'most', 'or', 'nowhere', 'forty', 'might', 'will', 'elsewhere', 'therein', 'see', 'put', 'others', 'beforehand', 'seems', 'many', 'nothing', 'next', 'two', 'before', 'keep', 'six', 'else', 'seeming', 'anything', 'out', 'whoever', 'either', 'are', 'few', 'just', 'me', 'hence', 'per', 'though', 'upon', 'after', 'does', 'if', 'serious', 'ten', 'ourselves', 'go', \"'re\", 'how', 'please', 'who', 'when', '’ll', 'being', 'moreover', 'himself', 'side', 'always', 'sometimes', 'than', 'not', 'very', 'ours', \"'s\", 'thereby', 'top', 'everything', 'amount', 'do', 'were', 'part', 're', 'a', 'he', 'to', 'used', 'mine', 'four', 'sixty', 'while', 'indeed', 'him', 'really', 'much', 'afterwards', 'which', 'whatever', 'during', 'less', 'say', '’m', 'with', 'in', 'sometime', 'under', 'one', 'but', 'some', 'perhaps', '‘ll', '‘ve', 'due', 'every', 'whereupon', 'by', 'anywhere', 'although', 'because', 'yet', 'neither', 'n‘t', 'this', 'why', 'along', 'namely', 'around', 'behind', 'also', 'throughout', 'using', 'other', 'of', 'made', \"'ll\", 'nobody', '‘d', 'get', 'since', 'something', 'formerly', 'again', 'beside', 'third', 'them', 'nevertheless', 'then', 'whom', 'except', 'up', 'unless', 'can', 'otherwise', 'name', 'that', \"'ve\", 'yourself', 'on', 'ca', 'together', 'once', 'whether', 'latter', 'enough', 'already', 'become', 'nine', 'take', 'against', 'they', 'within', 'been', 'toward', 'until', 'towards', 'now', 'anyway', 'these', 'only', 'latterly', 'your', 'none', 'whereby', 'we', 'whole', 'thru', 'it', 'thereafter', 'where', 'doing', 'twenty', 'and', 'n’t', 'whereafter', 'be', 'twelve', 'often', '’re', 'well', 'ever', 'both', 'about', 'still', 'almost', 'my', 'must', 'our', 'whence', 'asdf', 'former', 'eleven', 'have', 'fifteen', 'whereas', 'never', '’ve', 'his', 'rather', 'among', 'became', 'first', 'down', 'am', 'hereupon', 'mostly', 'hereafter', 'back', 'wherein', 'several', 'another', 'everyone', 'the', 'their', 'across', 'cannot', 'whose', 'an', 'show', 'i', 'seem', 'done', 'move', 'such', 'wherever', 'amongst', 'too', 'those', 'becoming', 'here', 'at', 'meanwhile', 'yours', 'everywhere', 'further', '’d', 'beyond', 'her', 'somewhere', 'thus', 'no', 'therefore', 'could', 'should', \"'m\", 'whither', 'besides', 'hers', 'hundred', 'same', 'any', 'nor', '‘s', 'over', 'more', 'eight', 'three'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rule-based Matcher\n"
      ],
      "metadata": {
        "id": "fOFnC0vjSmqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "_kJW3tjuSvgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Matcher library\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Here matcher is an object that pairs to current Vocab object\n",
        "# We can add and remove specific named matchers to matcher as needed\n"
      ],
      "metadata": {
        "id": "we5DwAtPqiob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating patterns**"
      ],
      "metadata": {
        "id": "pA3SIyosq21S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list and add a series of dictionaries to that list\n",
        "\n",
        "#Hello World can appear in the following ways:\n",
        "# 1) Hello-World\n",
        "# 2) Hello World (without hyphen)\n",
        "\n",
        "#pattern is a list of dictionaries.\n",
        "# In pattern 1 the key LOWER refers to executing the lowercase\n",
        " # operation on the token. The value 'hello' refers to the expected result of operating on the token\n",
        "pattern_1 = [{'LOWER' : 'hello'}, {'LOWER' : 'world'}]\n",
        "pattern_2 = [{'LOWER' : 'hello'}, {'IS_PUNCT' : True} , {'LOWER':'world'}]\n"
      ],
      "metadata": {
        "id": "I3T0TVw9rDB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add patterns to matcher object\n",
        "\n",
        "# Add a match rule to the matcher, wherein a match rule consists of:\n",
        "  # 1) An ID key\n",
        "  # 2) an on_match callback\n",
        "  # 3) one or more patterns\n",
        "\n",
        "matcher.add(key='Hello World', patterns=[pattern_1, pattern_2], on_match= None)"
      ],
      "metadata": {
        "id": "MVQlL1X0s06N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a document\n",
        "\n",
        "doc = nlp(\"'Hello World' is commonly the first printed phrase for most programmers. Printing 'Hello-World' is another variation of this.\")"
      ],
      "metadata": {
        "id": "VsOzffxFuUVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1dDQ8fruu5j",
        "outputId": "2a71255f-e335-4135-ae4d-c49760f325a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'\n",
            "Hello\n",
            "World\n",
            "'\n",
            "is\n",
            "commonly\n",
            "the\n",
            "first\n",
            "printed\n",
            "phrase\n",
            "for\n",
            "most\n",
            "programmers\n",
            ".\n",
            "Printing\n",
            "'\n",
            "Hello\n",
            "-\n",
            "World\n",
            "'\n",
            "is\n",
            "another\n",
            "variation\n",
            "of\n",
            "this\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding Matches ##\n",
        "\n",
        "Further reading: https://spacy.io/api/matcher"
      ],
      "metadata": {
        "id": "wUVmkuMUvAO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "find_matches = matcher(doc) #pass doc to the matcher and store this under find_matches variable\n",
        "print(find_matches)\n",
        "#printing finding matches returns a list of tuples\n",
        "  # each tuple has a string ID, index start and index end\n",
        "    #index start and end represent the token index in the document object\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLTLbXiLvVIq",
        "outputId": "5061fcaf-adef-4818-c81a-e8b5724440fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8585552006568828647, 1, 3), (8585552006568828647, 16, 19)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find and print the matching occurrences\n",
        "\n",
        "for match_id, start, end in find_matches:\n",
        "  string_id = nlp.vocab.strings[match_id] # get string representation\n",
        "  span = doc[start:end]\n",
        "  print(match_id, string_id, start, end, span.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9SDS_JQv5Y1",
        "outputId": "62c9e6ba-3877-487a-f7bb-a8b8f8ed9754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8585552006568828647 Hello World 1 3 Hello World\n",
            "8585552006568828647 Hello World 16 19 Hello-World\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phrase Matching ##"
      ],
      "metadata": {
        "id": "QlJ2q0_R2ESN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp_model = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "_8Bg2jxF2g8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import PhraseMatcher library\n",
        "from spacy.matcher import PhraseMatcher\n",
        "matcher = PhraseMatcher(nlp_model.vocab)"
      ],
      "metadata": {
        "id": "WZnhokZB2Jam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a list of phrases:\n",
        "phrase_list = [\"Barack Obama\" , \"Angela Merkel\" , \"Washington, D.C.\"]\n",
        "#Convert each phrase to a document object\n",
        "phrase_patterns = [nlp_model(text) for text in phrase_list]"
      ],
      "metadata": {
        "id": "vAYYeUzl2xRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_patterns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X02jpc6t3KzB",
        "outputId": "d0555b7f-666c-4313-bba9-eac5bfc8fcf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Barack Obama, Angela Merkel, Washington, D.C.]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(phrase_patterns[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GawSSUWp3UBm",
        "outputId": "e66562c6-670d-46bf-ede8-933a63caa1ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pass each doc object into the matcher\n",
        "matcher.add(\"TerminologyList\", on_match= None, docs=phrase_patterns)\n",
        "# \"TerminologyList\" is an alias given to matches that match the phrase_patterns"
      ],
      "metadata": {
        "id": "Q-bzCyrR3diC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_3 = nlp_model(text = \"German Chancellor Angela Merkel and US President Barack Obama \\\n",
        "converse in the Oval Office inside the White House in Washington, D.C.\")"
      ],
      "metadata": {
        "id": "aROR16dQ3aO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_matches = matcher(doc_3) #pass doc in to the matcher object and store it\n",
        "print(find_matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJj6UWab3Xrh",
        "outputId": "61ebee4f-7b31-4c68-f228-2d83e87a4e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(3766102292120407359, 2, 4), (3766102292120407359, 7, 9), (3766102292120407359, 19, 22)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#iterate over the matches and print the matched text\n",
        "\n",
        "for match_id, start_index, end_index in find_matches:\n",
        "  string_id = nlp_model.vocab.strings[match_id] #get string representation?\n",
        "  span = doc_3[start_index : end_index] #get the matched span\n",
        "  print(match_id, string_id, start_index, end_index, span.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpF4qwuQ5L1F",
        "outputId": "54864d94-a5fb-4195-88cc-0ec6c2f2dd22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3766102292120407359 TerminologyList 2 4 Angela Merkel\n",
            "3766102292120407359 TerminologyList 7 9 Barack Obama\n",
            "3766102292120407359 TerminologyList 19 22 Washington, D.C.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part of Speech \\(POS\\) Tagging ##\n",
        "\n",
        "After words are tokenized i.e ```nlp_model(text = \"Hey I just called the Apple Service Centre\") ```, they can be tagged by statstical models that enable spaCy to make predictions about what tag or label most likely applies in a given context. For example in the sentence above, the system would recognize Apple as a ```propernoun``` or ```PROPN```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WJJe3Frg6qRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "s2 = \"Hello! I just called the Apple Service Centre\""
      ],
      "metadata": {
        "id": "7fxIteYS_P6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp_model = spacy.load(name ='en_core_web_sm')"
      ],
      "metadata": {
        "id": "FIZGvkIm_l-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_1 = nlp_model(s1)\n",
        "doc_2 = nlp_model(s2)"
      ],
      "metadata": {
        "id": "Eh39Nv-Q67X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to get the readable string representation of an attribute, add an underscore suffix _ to its name:\n",
        "for token in doc_1:\n",
        "  print(token.text, token.pos_, token.tag_, spacy.explain(token.tag_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5iCTaNpAbs6",
        "outputId": "0dacfe40-0cc4-48a0-dfb0-3ab2b5435dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple PROPN NNP noun, proper singular\n",
            "is AUX VBZ verb, 3rd person singular present\n",
            "looking VERB VBG verb, gerund or present participle\n",
            "at ADP IN conjunction, subordinating or preposition\n",
            "buying VERB VBG verb, gerund or present participle\n",
            "U.K. PROPN NNP noun, proper singular\n",
            "startup NOUN NN noun, singular or mass\n",
            "for ADP IN conjunction, subordinating or preposition\n",
            "$ SYM $ symbol, currency\n",
            "1 NUM CD cardinal number\n",
            "billion NUM CD cardinal number\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc_2:\n",
        "  print(token.text, token.pos_, token.tag_, spacy.explain(token.tag_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS22CG_wAaxF",
        "outputId": "b4a499e4-deda-4b97-fb35-cf21b3e0eae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello INTJ UH interjection\n",
            "! PUNCT . punctuation mark, sentence closer\n",
            "I PRON PRP pronoun, personal\n",
            "just ADV RB adverb\n",
            "called VERB VBD verb, past tense\n",
            "the DET DT determiner\n",
            "Apple PROPN NNP noun, proper singular\n",
            "Service PROPN NNP noun, proper singular\n",
            "Centre PROPN NNP noun, proper singular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy"
      ],
      "metadata": {
        "id": "HmyFGGBx6xjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visually examine the dependent relationships between words in a sentence using displacy\n",
        "displacy.render(docs = doc_2, style = 'dep', jupyter= True, options = {\"distance\" : 130} )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "FaVupYRh6vA9",
        "outputId": "d6ba1663-f212-4441-ef17-f5ffa82fd98e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"835fc98a53744880ba6629c8d919b623-0\" class=\"displacy\" width=\"1090\" height=\"332.0\" direction=\"ltr\" style=\"max-width: none; height: 332.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"242.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Hello!</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">INTJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"242.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"180\">I</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"180\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"242.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"310\">just</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"310\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"242.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"440\">called</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"440\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"242.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"570\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"570\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"242.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"700\">Apple</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"700\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"242.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"830\">Service</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"830\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"242.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"960\">Centre</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"960\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-835fc98a53744880ba6629c8d919b623-0-0\" stroke-width=\"2px\" d=\"M200,197.0 C200,67.0 435.0,67.0 435.0,197.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-835fc98a53744880ba6629c8d919b623-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M200,199.0 L192,187.0 208,187.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-835fc98a53744880ba6629c8d919b623-0-1\" stroke-width=\"2px\" d=\"M330,197.0 C330,132.0 430.0,132.0 430.0,197.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-835fc98a53744880ba6629c8d919b623-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M330,199.0 L322,187.0 338,187.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-835fc98a53744880ba6629c8d919b623-0-2\" stroke-width=\"2px\" d=\"M590,197.0 C590,67.0 955.0,67.0 955.0,197.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-835fc98a53744880ba6629c8d919b623-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M590,199.0 L582,187.0 598,187.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-835fc98a53744880ba6629c8d919b623-0-3\" stroke-width=\"2px\" d=\"M720,197.0 C720,132.0 820.0,132.0 820.0,197.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-835fc98a53744880ba6629c8d919b623-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M720,199.0 L712,187.0 728,187.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-835fc98a53744880ba6629c8d919b623-0-4\" stroke-width=\"2px\" d=\"M850,197.0 C850,132.0 950.0,132.0 950.0,197.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-835fc98a53744880ba6629c8d919b623-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M850,199.0 L842,187.0 858,187.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-835fc98a53744880ba6629c8d919b623-0-5\" stroke-width=\"2px\" d=\"M460,197.0 C460,2.0 960.0,2.0 960.0,197.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-835fc98a53744880ba6629c8d919b623-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M960.0,199.0 L968.0,187.0 952.0,187.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition (NER)\n",
        "\n",
        "spaCy features an extremely fast statistical entity recognition system, that assigns labels to contiguous spans of tokens. The default trained pipelines can identify a variety of named and numeric entities, including companies, locations, organizations and products. Named entities are real-world objects that are assigned a name for example, a person, book or movie title or a product."
      ],
      "metadata": {
        "id": "CnVcd8ErDOyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = \"Facebook's CEO just announced a new language model called Gemini.\"\n",
        "s2 = \" My ancestors come from Sri Lanka's capital, Colombo.\"\n",
        "s3 = \" I've found that Apple macbooks are highly performant.\""
      ],
      "metadata": {
        "id": "7O6JjhKJDX9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp_model = spacy.load(name = 'en_core_web_sm')"
      ],
      "metadata": {
        "id": "iw41BheVD4nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_1 = nlp_model(s1)\n",
        "doc_2 = nlp_model(s2)\n",
        "doc_3 = nlp_model(s3)"
      ],
      "metadata": {
        "id": "qTUCF509EBn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#identify the entities in each doc\n",
        "\n",
        "for entity in doc_1.ents:\n",
        "  print(entity, entity.label_,str(spacy.explain(entity.label_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtiVKLm2FIcS",
        "outputId": "41672218-c27f-448e-8412-fbbefc5c8b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini PRODUCT Objects, vehicles, foods, etc. (not services)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding an entity to recognize\n",
        "\n",
        "Notice that the system does not recognize *Facebook* in s1 as an entity. To add a new entity we can take the following steps:"
      ],
      "metadata": {
        "id": "52f31lBRUnna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import Span\n",
        "ORG = doc_1.vocab.strings['ORG']"
      ],
      "metadata": {
        "id": "_di0ia6_W4X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The span function takes 4 arguements.\n",
        "  #1) The document from which you will define a new entity\n",
        "  # 2) The starting index of the token with the document\n",
        "  # 3) The ending index of the token within the document\n",
        "  # 4) The label for the entity i.e ORG\n",
        "new_ent = Span(doc_1, 0,1,label = ORG)"
      ],
      "metadata": {
        "id": "LLxQzt30VwyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_1.ents = list(doc_1.ents) +[new_ent]"
      ],
      "metadata": {
        "id": "cj0a84bMUwDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display entities in doc 1 after the addition\n",
        "doc_1.ents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUjGPM5MXIWt",
        "outputId": "7afc1d48-4f07-4110-ae18-b1d124fe815c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Facebook, Gemini)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We can use displacy to visually display the different entities in a doc:\n",
        "from spacy import displacy\n",
        "\n",
        "displacy.render(docs= [doc_1,doc_2,doc_3], style = 'ent', jupyter= True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "UMgRc37yXl3e",
        "outputId": "a2ca92ba-5acd-4f2c-d6ea-2847a5f9a7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Facebook\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "'s CEO just announced a new language model called \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Gemini\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              ".</div>\n",
              "\n",
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"> My ancestors come from \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Sri Lanka's\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " capital, \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Colombo\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ".</div>\n",
              "\n",
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"> I've found that \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " macbooks are highly performant.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can use the 'options' argument in the render function to display entities of a certain type:\n",
        "#In this case only orgs or products\n",
        "\n",
        "displacy.render(docs= [doc_1,doc_2,doc_3], style = 'ent',options= {'ents': ['ORG','PRODUCT']}, jupyter= True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "ihhMX6ayX99F",
        "outputId": "571fa3c9-32ab-4dc2-808a-f7388540b2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Facebook\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "'s CEO just announced a new language model called \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Gemini\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              ".</div>\n",
              "\n",
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"> My ancestors come from Sri Lanka's capital, Colombo.</div>\n",
              "\n",
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"> I've found that \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " macbooks are highly performant.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the system didn't pick up macbook as a product. Let's add that as an entity:"
      ],
      "metadata": {
        "id": "9CQcXGROZQ_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tokens in nlp_model(s3):\n",
        "  print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG4YDzDGaBNd",
        "outputId": "f245572b-5203-48e1-b57a-3dd4fc1fcb74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "I\n",
            "'ve\n",
            "found\n",
            "that\n",
            "Apple\n",
            "macbooks\n",
            "are\n",
            "highly\n",
            "performant\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PRODUCT = doc_3.vocab.strings['PRODUCT']\n",
        "#Define a new entity from the string\n",
        "new_ent = Span(doc_3, 6,7,label = PRODUCT)\n",
        "#Add the new entity\n",
        "doc_3.ents = list(doc_3.ents) + [new_ent]"
      ],
      "metadata": {
        "id": "3ftzhi_VZQOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(docs= [doc_1,doc_2,doc_3], style = 'ent',options= {'ents': ['ORG','PRODUCT']}, jupyter= True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "h5EN8UazaWdM",
        "outputId": "3c64a08c-277b-4232-c90d-d752f344dc74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Facebook\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "'s CEO just announced a new language model called \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Gemini\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              ".</div>\n",
              "\n",
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"> My ancestors come from Sri Lanka's capital, Colombo.</div>\n",
              "\n",
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"> I've found that \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    macbooks\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              " are highly performant.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Segmentation"
      ],
      "metadata": {
        "id": "Cg6SUmvscUKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = \"This is a sentence. This is a second sentence. This is the last sentence.\"\n",
        "s2 = \"This is a sentence; This is a second sentence; This is the last sentence;\""
      ],
      "metadata": {
        "id": "yw21FWdQcXCc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp_model = spacy.load(name = 'en_core_web_sm')"
      ],
      "metadata": {
        "id": "JHdjqnQHco0h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_1 = nlp_model(s1)\n",
        "\n",
        "#To see if there are sentences identified by spacy we can use the sents property\n",
        "doc_1.sents\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7MWgXagc2YH",
        "outputId": "40d10cb3-edbf-427a-ec0c-b41935992d21"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator at 0x78320b6fd3a0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sents in doc_1.sents:\n",
        "  print(sents.text)"
      ],
      "metadata": {
        "id": "PB_ZT7kQdTVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2185096f-edb4-4c4f-d690-eabb0b24e726"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a sentence.\n",
            "This is a second sentence.\n",
            "This is the last sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One advantage spacy has over using regex is that it can use entity recognition within the sentence segmentation. For example:\n"
      ],
      "metadata": {
        "id": "v5KdBRw-doUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s3 = \"Mr. Ray lives in Washington D.C. with his mother. They have a net-worth of 3.4 million.\" #lol I wish\n",
        "doc_3 = nlp_model(s3)\n",
        "\n",
        "for sents in doc_3.sents:\n",
        "  print(sents.text)"
      ],
      "metadata": {
        "id": "Z2BFSk5Udw2Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02d47aa3-6305-4d5e-d752-6e78baf3d2a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mr. Ray lives in Washington D.C. with his mother.\n",
            "They have a net-worth of 3.4 million.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_2 = nlp_model(s2)\n",
        "for sents in doc_2.sents:\n",
        "  print(sents.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az1Kw5JpkCXS",
        "outputId": "66ac66ce-8de2-44a3-c39a-1c0b0e3f3874"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a sentence; This is a second sentence; This is the last sentence;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Custom Pipeline Components\n",
        "\n",
        "Check out this documentation for more on 2 methods to add custom components: https://spacy.io/usage/processing-pipelines#custom-components"
      ],
      "metadata": {
        "id": "e8T8R463rugg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that spacy was not able to segment sentences separated by a semi-colon"
      ],
      "metadata": {
        "id": "5Zrf_03vksaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a custom function. Use the @Language.component decorator to turn a function\n",
        "# into a pipeline component\n",
        "from spacy.language import Language\n",
        "@Language.component(\"custom_splitter\")\n",
        "def set_custom_boundaries(doc):\n",
        "  for token in doc[:-1]:\n",
        "    if token.text == ';':\n",
        "      print(token.i)\n",
        "      doc[token.i+1].is_sent_start = True\n",
        "      return doc"
      ],
      "metadata": {
        "id": "JtqFHJFSk2s-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we run ```nlp_model(s3)``` we create a document with a number of attributes. This involves a step by step pipeline.To see the pipeline we can print the functions out. We can then add the custom function to the nlp processing pipeline.\n",
        "\n",
        ">When you call nlp on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. The pipeline used by the trained pipelines typically include a tagger, a lemmatizer, a parser and an entity recognizer. Each pipeline component returns the processed Doc, which is then passed on to the next component.\n",
        "\n",
        "Resource: https://spacy.io/usage/spacy-101#pipelines\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SZUGKYpwlwqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_model.pipe_names"
      ],
      "metadata": {
        "id": "PL0tZxKylmhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want the set_custom_boundaries function to be after the tokenizer and the before the parser because the tokenizer splits text into tokens and the parser assigns dependency labels. We want to split the sentence so that the parser will assign dependency within individual sentences and not the large ensemble sentence that exists before the split.\n"
      ],
      "metadata": {
        "id": "lC_yJ88dl_pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_model.add_pipe('custom_splitter', before ='parser')"
      ],
      "metadata": {
        "id": "71-cxY0opf2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_model.pipe_names\n",
        "#Notice how the new 'custom_splitter' pipeline component has been added"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B21UJ9DxsHVR",
        "outputId": "ab9a3c84-51c5-4d1d-b540-886ec129805a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec',\n",
              " 'tagger',\n",
              " 'custom_splitter',\n",
              " 'parser',\n",
              " 'attribute_ruler',\n",
              " 'lemmatizer',\n",
              " 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_2 = nlp_model(s2)\n",
        "for sent in doc_2.sents:\n",
        "  print(sent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UnC2DuNwgmr",
        "outputId": "ec3b0668-c4b5-48a7-de7b-29797141fafc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "This is a sentence;\n",
            "This is a second sentence; This is the last sentence;\n"
          ]
        }
      ]
    }
  ]
}